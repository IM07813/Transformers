{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNHd003WxcuyKEBKskBOzYX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Dwci8PvMMpi",
        "outputId": "c3d2ba2d-40b0-499c-f9ca-dde8d166be6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Load the IMDB dataset\n",
        "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.imdb.load_data(num_words=10000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(sequences, maxlen, padding='post'):\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen, padding=padding)\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "train_data = preprocess_data(train_data, maxlen)\n",
        "test_data = preprocess_data(test_data, maxlen)\n"
      ],
      "metadata": {
        "id": "YnyUI2GYNccx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(tf.keras.Model):\n",
        "    def __init__(self, num_heads, num_layers, d_model, vocab_size, dff, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=d_model)\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
        "        self.dense1 = layers.Dense(units=d_model)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout)\n",
        "        self.ffn = layers.Dense(units=dff, activation='relu')\n",
        "        self.dense2 = layers.Dense(units=d_model)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout2 = layers.Dropout(dropout)\n",
        "        self.global_pool = layers.GlobalAveragePooling1D()\n",
        "        self.fc_out = layers.Dense(units=1, activation='sigmoid')\n",
        "\n",
        "    def call(self, x, training):\n",
        "        x = self.embedding(x)\n",
        "        attn_output = self.attention(x, x)\n",
        "        x1 = self.dense1(attn_output)\n",
        "        x = self.norm1(x + x1)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        ffn_output = self.ffn(x)\n",
        "        x2 = self.dense2(ffn_output)\n",
        "        x = self.norm2(x + x2)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.global_pool(x)\n",
        "        return self.fc_out(x)\n",
        "\n",
        "# Define the model architecture\n",
        "num_heads = 2\n",
        "num_layers = 2\n",
        "d_model = 64\n",
        "vocab_size = 10000\n",
        "dff = 512\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerModel(num_heads, num_layers, d_model, vocab_size, dff, dropout)\n",
        "\n"
      ],
      "metadata": {
        "id": "QEssNPINNepF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "epochs = 10\n",
        "history = model.fit(train_data, train_labels, epochs=epochs, validation_data=(test_data, test_labels), batch_size=64)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYY0u_qPNrTC",
        "outputId": "ba16b6d0-5a1b-4c11-fa3a-0f0f8d6152cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 50s 105ms/step - loss: 0.4064 - accuracy: 0.8100 - val_loss: 0.3446 - val_accuracy: 0.8471\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 12s 32ms/step - loss: 0.2616 - accuracy: 0.8931 - val_loss: 0.3620 - val_accuracy: 0.8448\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 9s 24ms/step - loss: 0.2181 - accuracy: 0.9160 - val_loss: 0.4089 - val_accuracy: 0.8319\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1884 - accuracy: 0.9272 - val_loss: 0.4561 - val_accuracy: 0.8251\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.1602 - accuracy: 0.9408 - val_loss: 0.5359 - val_accuracy: 0.8246\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 9s 22ms/step - loss: 0.1335 - accuracy: 0.9483 - val_loss: 0.6192 - val_accuracy: 0.8154\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 8s 20ms/step - loss: 0.1035 - accuracy: 0.9568 - val_loss: 0.6787 - val_accuracy: 0.8161\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 7s 19ms/step - loss: 0.0827 - accuracy: 0.9630 - val_loss: 0.7836 - val_accuracy: 0.8129\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 6s 16ms/step - loss: 0.0642 - accuracy: 0.9726 - val_loss: 0.9341 - val_accuracy: 0.8057\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 6s 15ms/step - loss: 0.0525 - accuracy: 0.9797 - val_loss: 0.9835 - val_accuracy: 0.8054\n"
          ]
        }
      ]
    }
  ]
}