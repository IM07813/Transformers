{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.data = [\n",
        "            \"The cat sits on the mat\",\n",
        "            \"Dogs love to chase balls\",\n",
        "            \"Children play in the park\",\n",
        "            \"Birds sing in the trees\",\n",
        "            \"She reads a good book\",\n",
        "        ]\n",
        "        self.max_length = 12  # Including <sos> and <eos>\n",
        "        self.batch_size = 2\n",
        "        self.epochs = 100\n",
        "        self.d_model = 128\n",
        "        self.n_heads = 4\n",
        "        self.n_layers = 4\n",
        "        self.ffn_dim = 512\n",
        "        self.dropout = 0.1\n",
        "        self.lr = 5e-4\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Vocabulary implementation\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.special_tokens = [config.pad_token, config.sos_token,\n",
        "                              config.eos_token, config.unk_token]\n",
        "        self.word2idx = {t:i for i,t in enumerate(self.special_tokens)}\n",
        "        self.idx2word = self.special_tokens.copy()\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.lower().split():\n",
        "                if word not in self.word2idx:\n",
        "                    self.word2idx[word] = len(self.idx2word)\n",
        "                    self.idx2word.append(word)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "# Dataset preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, vocab):\n",
        "        self.vocab = vocab\n",
        "        self.data = []\n",
        "        for sentence in config.data:\n",
        "            tokens = [config.sos_token] + sentence.lower().split() + [config.eos_token]\n",
        "            indices = [self.vocab.word2idx.get(word, self.vocab.word2idx[config.unk_token])\n",
        "                      for word in tokens]\n",
        "            if len(indices) > config.max_length:\n",
        "                indices = indices[:config.max_length]\n",
        "            self.data.append(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        pad_len = config.max_length - len(seq)\n",
        "        inputs = seq[:-1] + [self.vocab.word2idx[config.pad_token]] * (pad_len + 1)\n",
        "        targets = seq[1:] + [self.vocab.word2idx[config.pad_token]] * (pad_len + 1)\n",
        "        return (\n",
        "            torch.tensor(inputs[:config.max_length-1], dtype=torch.long),\n",
        "            torch.tensor(targets[:config.max_length-1], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "# Positional Embeddings\n",
        "class RotaryPositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, n_heads, seq_len, _ = x.shape\n",
        "        t = torch.arange(seq_len, device=x.device, dtype=torch.float32)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        cos = emb.cos()[None, None, :, :]\n",
        "        sin = emb.sin()[None, None, :, :]\n",
        "        x_rot = torch.cat([-x[..., self.dim//2:], x[..., :self.dim//2]], dim=-1)\n",
        "        return (x * cos) + (x_rot * sin)\n",
        "\n",
        "# Transformer Components\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.d_model = config.d_model\n",
        "        self.n_heads = config.n_heads\n",
        "        self.head_dim = self.d_model // self.n_heads\n",
        "\n",
        "        self.qkv = nn.Linear(self.d_model, 3*self.d_model)\n",
        "        self.out = nn.Linear(self.d_model, self.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.rotary = RotaryPositionalEncoding(self.head_dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, T, _ = x.shape\n",
        "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = [t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2) for t in qkv]\n",
        "\n",
        "        # Apply Rotary positional embeddings\n",
        "        q, k = self.rotary(q), self.rotary(k)\n",
        "\n",
        "        # Adjust mask for attention\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]\n",
        "            mask = mask.expand(B, self.n_heads, -1, -1)\n",
        "\n",
        "        attn = F.scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            attn_mask=mask,\n",
        "            dropout_p=config.dropout if self.training else 0\n",
        "        )\n",
        "\n",
        "        attn = attn.transpose(1, 2).contiguous().view(B, T, self.d_model)\n",
        "        return self.dropout(self.out(attn))\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.d_model, config.ffn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(config.ffn_dim, config.d_model),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.attn_norm = nn.LayerNorm(config.d_model)\n",
        "        self.ffn_norm = nn.LayerNorm(config.d_model)\n",
        "        self.attn = MultiHeadAttention()\n",
        "        self.ffn = FeedForward()\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.attn(self.attn_norm(x), mask)\n",
        "        x = x + self.ffn(self.ffn_norm(x))\n",
        "        return x\n",
        "\n",
        "# Main Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        self.vocab = vocab\n",
        "        self.token_emb = nn.Embedding(len(vocab), config.d_model)\n",
        "        self.layers = nn.ModuleList([TransformerBlock() for _ in range(config.n_layers)])\n",
        "        self.norm = nn.LayerNorm(config.d_model)\n",
        "        self.head = nn.Linear(config.d_model, len(vocab))\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.token_emb.weight.data.normal_(mean=0.0, std=0.02)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.token_emb(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.head(self.norm(x))\n",
        "\n",
        "# Training Utilities\n",
        "def create_mask(size):\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "    return mask\n",
        "\n",
        "def train_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    vocab = Vocabulary()\n",
        "    vocab.build_vocab(config.data)\n",
        "    dataset = TextDataset(vocab)\n",
        "    loader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    model = Transformer(vocab).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[config.pad_token])\n",
        "    mask = create_mask(config.max_length-1).to(device)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits = model(inputs, mask)\n",
        "            loss = criterion(logits.view(-1, len(vocab)), targets.view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{config.epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model, vocab\n",
        "\n",
        "# Generation Function\n",
        "@torch.no_grad()\n",
        "def generate(model, vocab, prompt, max_len=20):\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    tokens = [vocab.word2idx.get(word.lower(), vocab.word2idx[config.unk_token])\n",
        "             for word in prompt.split()]\n",
        "    tokens = [vocab.word2idx[config.sos_token]] + tokens\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        if len(tokens) >= config.max_length:\n",
        "            break\n",
        "\n",
        "        input_seq = torch.tensor(tokens[-config.max_length+1:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        mask = create_mask(len(input_seq[0])).to(device)\n",
        "        logits = model(input_seq, mask)\n",
        "        next_token = logits[0, -1].argmax().item()\n",
        "        tokens.append(next_token)\n",
        "        if next_token == vocab.word2idx[config.eos_token]:\n",
        "            break\n",
        "\n",
        "    output = []\n",
        "    for t in tokens[1:]:  # Skip <sos>\n",
        "        if t == vocab.word2idx[config.eos_token]:\n",
        "            break\n",
        "        if t not in [vocab.word2idx[config.sos_token], vocab.word2idx[config.pad_token]]:\n",
        "            output.append(vocab.idx2word[t])\n",
        "    return ' '.join(output).capitalize()\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting training...\")\n",
        "    model, vocab = train_model()\n",
        "\n",
        "    print(\"\\nSample Generations:\")\n",
        "    test_prompts = [\"the\", \"dogs\", \"children\"]\n",
        "    for prompt in test_prompts:\n",
        "        generated = generate(model, vocab, prompt)\n",
        "        print(f\"'{prompt}' → {generated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyB3MfVqXj_p",
        "outputId": "31613e27-80bf-4699-f15f-af430f2c91dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Using device: cpu\n",
            "Epoch 10/100 | Loss: 1.4878\n",
            "Epoch 20/100 | Loss: 0.8245\n",
            "Epoch 30/100 | Loss: 0.5104\n",
            "Epoch 40/100 | Loss: 0.3831\n",
            "Epoch 50/100 | Loss: 0.3342\n",
            "Epoch 60/100 | Loss: 0.3084\n",
            "Epoch 70/100 | Loss: 0.2879\n",
            "Epoch 80/100 | Loss: 0.2850\n",
            "Epoch 90/100 | Loss: 0.2756\n",
            "Epoch 100/100 | Loss: 0.2875\n",
            "\n",
            "Sample Generations:\n",
            "'the' → The cat sits on the mat\n",
            "'dogs' → Dogs love to chase balls\n",
            "'children' → Children play in the park\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import random\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "def main():\n",
        "    print(\"Initializing...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize vocabulary and data\n",
        "    vocab = Vocabulary()\n",
        "    vocab.build_vocab([ex[1] for ex in config.few_shot_examples])\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = TextDataset(vocab, config.train_data)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = Transformer(len(vocab)).to(device)\n",
        "    print(f\"\\nTraining model with {sum(p.numel() for p in model.parameters())} parameters...\")\n",
        "\n",
        "    model, losses = train_model(model, train_loader, vocab, device)\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "    # Evaluate on few-shot examples and generate new samples\n",
        "    evaluate_few_shot(model, vocab, device, config.few_shot_examples)\n",
        "\n",
        "# Replace just the Config class with this updated version:\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        # Training data\n",
        "        self.train_data = [\n",
        "            \"The cat sits on the mat\",\n",
        "            \"Dogs love to chase balls\",\n",
        "            \"Children play in the park\",\n",
        "            \"Birds sing in the trees\",\n",
        "            \"She reads a good book\",\n",
        "            \"The sun shines brightly today\",\n",
        "            \"Students study in the library\",\n",
        "            \"Fish swim in the ocean\",\n",
        "            \"The wind blows through leaves\",\n",
        "            \"People walk in the garden\"\n",
        "        ]\n",
        "\n",
        "        # Few-shot examples (not used in training)\n",
        "        self.few_shot_examples = [\n",
        "            (\"The mouse\", \"The mouse runs through the house\"),\n",
        "            (\"A teacher\", \"A teacher explains the lesson to students\"),\n",
        "            (\"The car\", \"The car drives down the street quickly\"),\n",
        "            (\"In winter\", \"In winter snow falls softly on the ground\"),\n",
        "            (\"The coffee\", \"The coffee steams in the morning light\")\n",
        "        ]\n",
        "\n",
        "        # Model configuration\n",
        "        self.max_length = 16  # Increased for longer sequences\n",
        "        self.batch_size = 4\n",
        "        self.epochs = 150\n",
        "        self.d_model = 256  # Increased model capacity\n",
        "        self.n_heads = 8\n",
        "        self.n_layers = 6\n",
        "        self.ffn_dim = 1024\n",
        "        self.dropout = 0.1\n",
        "        self.lr = 3e-4\n",
        "\n",
        "        # Special tokens\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "\n",
        "        # Training settings\n",
        "        self.warmup_steps = 100  # Added missing parameter\n",
        "        self.gradient_clip = 1.0\n",
        "        self.eval_every = 10\n",
        "        self.min_lr = 1e-9  # Added minimum learning rate\n",
        "        self.weight_decay = 0.01  # Added weight decay\n",
        "        self.max_grad_norm = 1.0  # Added maximum gradient norm\n",
        "\n",
        "        # Generation settings\n",
        "        self.temperature = 0.7\n",
        "        self.max_gen_length = 30\n",
        "\n",
        "        # Evaluation settings\n",
        "        self.num_samples = 3\n",
        "\n",
        "config = Config()\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.special_tokens = [config.pad_token, config.sos_token,\n",
        "                             config.eos_token, config.unk_token]\n",
        "        self.word2idx = {t:i for i,t in enumerate(self.special_tokens)}\n",
        "        self.idx2word = self.special_tokens.copy()\n",
        "\n",
        "    def build_vocab(self, sentences):\n",
        "        for sentence in sentences:\n",
        "            for word in sentence.lower().split():\n",
        "                if word not in self.word2idx:\n",
        "                    self.word2idx[word] = len(self.idx2word)\n",
        "                    self.idx2word.append(word)\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        return [self.word2idx.get(word.lower(), self.word2idx[config.unk_token])\n",
        "                for word in text.split()]\n",
        "\n",
        "    def decode(self, indices: List[int]) -> str:\n",
        "        return ' '.join([self.idx2word[idx] for idx in indices\n",
        "                        if idx not in [self.word2idx[t] for t in [config.pad_token, config.sos_token, config.eos_token]]])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, vocab: Vocabulary, data: List[str]):\n",
        "        self.vocab = vocab\n",
        "        self.data = []\n",
        "\n",
        "        for sentence in data:\n",
        "            tokens = [config.sos_token] + sentence.lower().split() + [config.eos_token]\n",
        "            indices = [self.vocab.word2idx.get(word, self.vocab.word2idx[config.unk_token])\n",
        "                      for word in tokens]\n",
        "            if len(indices) > config.max_length:\n",
        "                indices = indices[:config.max_length]\n",
        "            self.data.append(indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq = self.data[idx]\n",
        "        pad_len = config.max_length - len(seq)\n",
        "        inputs = seq[:-1] + [self.vocab.word2idx[config.pad_token]] * (pad_len + 1)\n",
        "        targets = seq[1:] + [self.vocab.word2idx[config.pad_token]] * (pad_len + 1)\n",
        "        return (\n",
        "            torch.tensor(inputs[:config.max_length-1], dtype=torch.long),\n",
        "            torch.tensor(targets[:config.max_length-1], dtype=torch.long)\n",
        "        )\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(config.d_model, config.n_heads,\n",
        "                                             dropout=config.dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(config.d_model)\n",
        "        self.norm2 = nn.LayerNorm(config.d_model)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(config.d_model, config.ffn_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.ffn_dim, config.d_model)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output, _ = self.attention(x, x, x, attn_mask=mask)\n",
        "        x = self.norm1(x + self.dropout(attention_output))\n",
        "        feedforward_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(feedforward_output))\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, config.d_model)\n",
        "        self.position_encoding = PositionalEncoding(config.d_model)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock() for _ in range(config.n_layers)]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(config.d_model)\n",
        "        self.fc_out = nn.Linear(config.d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.dropout(self.position_encoding(self.token_embedding(x)))\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, mask)\n",
        "        return self.fc_out(self.norm(x))\n",
        "\n",
        "def create_mask(size: int) -> torch.Tensor:\n",
        "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
        "    mask = mask.masked_fill(mask == 1, float('-inf'))\n",
        "    return mask\n",
        "\n",
        "class WarmupScheduler:\n",
        "    def __init__(self, optimizer, d_model, warmup_steps):\n",
        "        self.optimizer = optimizer\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.current_step = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.current_step += 1\n",
        "        lr = self.d_model ** (-0.5) * min(self.current_step ** (-0.5),\n",
        "                                         self.current_step * self.warmup_steps ** (-1.5))\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "def train_model(model: nn.Module, train_loader: DataLoader, vocab: Vocabulary,\n",
        "                device: torch.device) -> Tuple[nn.Module, List[float]]:\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab.word2idx[config.pad_token])\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98))\n",
        "    scheduler = WarmupScheduler(optimizer, config.d_model, config.warmup_steps)\n",
        "\n",
        "    losses = []\n",
        "    mask = create_mask(config.max_length-1).to(device)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs, mask)\n",
        "            loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        if (epoch + 1) % config.eval_every == 0:\n",
        "            print(f\"Epoch {epoch+1}/{config.epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Generate a sample during training\n",
        "            if (epoch + 1) % (config.eval_every * 2) == 0:\n",
        "                model.eval()\n",
        "                sample_prompt = random.choice(config.few_shot_examples)[0]\n",
        "                generated = generate(model, vocab, sample_prompt, device)\n",
        "                print(f\"Sample generation: '{sample_prompt}' → {generated}\\n\")\n",
        "\n",
        "    return model, losses\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model: nn.Module, vocab: Vocabulary, prompt: str, device: torch.device,\n",
        "            max_len: int = 30, temperature: float = 0.7) -> str:\n",
        "    model.eval()\n",
        "    tokens = [vocab.word2idx[config.sos_token]] + vocab.encode(prompt)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        if len(tokens) >= config.max_length:\n",
        "            break\n",
        "\n",
        "        input_seq = torch.tensor(tokens[-config.max_length+1:], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        mask = create_mask(len(input_seq[0])).to(device)\n",
        "\n",
        "        logits = model(input_seq, mask)\n",
        "        logits = logits[0, -1, :] / temperature\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "        tokens.append(next_token)\n",
        "        if next_token == vocab.word2idx[config.eos_token]:\n",
        "            break\n",
        "\n",
        "    return vocab.decode(tokens[1:])  # Skip <sos>\n",
        "\n",
        "def evaluate_few_shot(model: nn.Module, vocab: Vocabulary, device: torch.device,\n",
        "                     examples: List[Tuple[str, str]], num_samples: int = 3) -> None:\n",
        "    model.eval()\n",
        "    print(\"\\nFew-shot Generation Examples:\")\n",
        "\n",
        "    for prompt, target in examples[:num_samples]:\n",
        "        generated = generate(model, vocab, prompt, device)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "        print(f\"Target: {target}\")\n",
        "\n",
        "    # Generate with new unseen prompts\n",
        "    print(\"\\nGeneration with Unseen Prompts:\")\n",
        "    new_prompts = [\n",
        "        \"The rainbow\",\n",
        "        \"In space\",\n",
        "        \"The robot\",\n",
        "        \"During summer\",\n",
        "        \"The musician\"\n",
        "    ]\n",
        "\n",
        "    for prompt in new_prompts:\n",
        "        generated = generate(model, vocab, prompt, device)\n",
        "        print(f\"\\nPrompt: {prompt}\")\n",
        "        print(f\"Generated: {generated}\")\n",
        "\n",
        "def main():\n",
        "    print(\"Initializing...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize vocabulary and data\n",
        "    vocab = Vocabulary()\n",
        "    vocab.build_vocab([ex[1] for ex in config.few_shot_examples])\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = TextDataset(vocab, config.train_data)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = Transformer(len(vocab)).to(device)\n",
        "    print(f\"\\nTraining model with {sum(p.numel() for p in model.parameters())} parameters...\")\n",
        "\n",
        "    model, losses = train_model(model, train_loader, vocab, device)\n",
        "    print(\"\\nTraining completed!\")\n",
        "\n",
        "    # Evaluate on few-shot examples and generate new samples\n",
        "    evaluate_few_shot(model, vocab, device, config.few_shot_examples)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmxSC7zoY_nJ",
        "outputId": "eebb319e-86eb-46fd-f6ac-dab87aa6fc08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing...\n",
            "Using device: cpu\n",
            "\n",
            "Training model with 4754975 parameters...\n",
            "Epoch 10/150 | Loss: 0.7461\n",
            "Epoch 20/150 | Loss: 1.3769\n",
            "Sample generation: 'In winter' → in winter <unk> <unk>\n",
            "\n",
            "Epoch 30/150 | Loss: 1.4621\n",
            "Epoch 40/150 | Loss: 1.4615\n",
            "Sample generation: 'The coffee' → the coffee <unk> <unk> <unk> <unk> the the <unk> <unk> <unk> <unk> through <unk> <unk>\n",
            "\n",
            "Epoch 50/150 | Loss: 1.4704\n",
            "Epoch 60/150 | Loss: 1.4679\n",
            "Sample generation: 'The coffee' → the coffee\n",
            "\n",
            "Epoch 70/150 | Loss: 1.5305\n",
            "Epoch 80/150 | Loss: 1.4937\n",
            "Sample generation: 'The car' → the car <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> the the <unk> <unk> <unk>\n",
            "\n",
            "Epoch 90/150 | Loss: 1.5428\n",
            "Epoch 100/150 | Loss: 1.4665\n",
            "Sample generation: 'A teacher' → a teacher the <unk> <unk> <unk> <unk> in in <unk> <unk>\n",
            "\n",
            "Epoch 110/150 | Loss: 1.4065\n",
            "Epoch 120/150 | Loss: 1.4934\n",
            "Sample generation: 'The coffee' → the coffee <unk> <unk> <unk> <unk> <unk> <unk> students <unk> <unk>\n",
            "\n",
            "Epoch 130/150 | Loss: 1.4496\n",
            "Epoch 140/150 | Loss: 1.3783\n",
            "Sample generation: 'The car' → the car <unk> <unk> <unk> <unk> <unk> <unk> the <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "Epoch 150/150 | Loss: 1.4293\n",
            "\n",
            "Training completed!\n",
            "\n",
            "Few-shot Generation Examples:\n",
            "\n",
            "Prompt: The mouse\n",
            "Generated: the mouse\n",
            "Target: The mouse runs through the house\n",
            "\n",
            "Prompt: A teacher\n",
            "Generated: a teacher <unk> <unk>\n",
            "Target: A teacher explains the lesson to students\n",
            "\n",
            "Prompt: The car\n",
            "Generated: the car\n",
            "Target: The car drives down the street quickly\n",
            "\n",
            "Generation with Unseen Prompts:\n",
            "\n",
            "Prompt: The rainbow\n",
            "Generated: the <unk>\n",
            "\n",
            "Prompt: In space\n",
            "Generated: in <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> on <unk> <unk> <unk> <unk>\n",
            "\n",
            "Prompt: The robot\n",
            "Generated: the <unk> the <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "Prompt: During summer\n",
            "Generated: <unk> <unk> <unk> the <unk> <unk> <unk> <unk> <unk> the in\n",
            "\n",
            "Prompt: The musician\n",
            "Generated: the <unk> <unk> <unk> <unk>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n8nwxHSCXq7U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}