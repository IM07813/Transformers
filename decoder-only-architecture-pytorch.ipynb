{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Synthetic Input Generation\ninput_tensor = torch.randint(0, 10000, (2, 8))  # (batch_size, seq_len)\n\n# 2. Model \nclass MultiHeadAttention(nn.Module):\n    \"\"\"Scaled dot-product attention with causal masking\"\"\"\n    def __init__(self, d_model=512, n_heads=8):\n        super().__init__()\n        self.d_head = d_model // n_heads\n        self.n_heads = n_heads\n        self.qkv_proj = nn.Linear(d_model, 3*d_model)\n        self.out_proj = nn.Linear(d_model, d_model)\n\n    def forward(self, x, mask):\n        B, T, C = x.size()\n        q, k, v = self.qkv_proj(x).split(C, dim=2)\n        q = q.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)\n\n        attn = (q @ k.transpose(-2, -1)) * (1.0 / (self.d_head ** 0.5))\n        attn = attn.masked_fill(mask, float('-inf'))\n        attn = F.softmax(attn, dim=-1)\n        x = (attn @ v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.out_proj(x)\n\nclass FeedForward(nn.Module):\n    \"\"\"Position-wise FFN with GELU activation\"\"\"\n    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Single decoder layer with pre-norm architecture\"\"\"\n    def __init__(self, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n        super().__init__()\n        self.attn_norm = nn.LayerNorm(d_model)\n        self.ffn_norm = nn.LayerNorm(d_model)\n        self.attn = MultiHeadAttention(d_model, n_heads)\n        self.ffn = FeedForward(d_model, d_ff, dropout)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        x = x + self.dropout(self.attn(self.attn_norm(x), mask))\n        x = x + self.dropout(self.ffn(self.ffn_norm(x)))\n        return x\n\nclass Transformer(nn.Module):\n    \"\"\"Full decoder-only Transformer\"\"\"\n    def __init__(self, vocab_size=10000, d_model=512, n_heads=8,\n                 d_ff=2048, num_layers=6, max_seq_len=512, dropout=0.1):\n        super().__init__()\n        self.tok_emb = nn.Embedding(vocab_size, d_model)\n        self.pos_emb = nn.Embedding(max_seq_len, d_model)\n        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff, dropout)\n                                   for _ in range(num_layers)])\n        self.norm = nn.LayerNorm(d_model)\n        self.out = nn.Linear(d_model, vocab_size)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_normal_(module.weight)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def forward(self, x):\n        B, T = x.size()\n        pos = torch.arange(T, device=x.device).unsqueeze(0)\n        x = self.tok_emb(x) + self.pos_emb(pos)\n        mask = torch.triu(torch.ones(T, T, dtype=torch.bool, device=x.device), diagonal=1)\n        mask = mask.view(1, 1, T, T)  # (1, 1, T, T)\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.out(self.norm(x))\n\n# 3. RUN\nmodel = Transformer(vocab_size=10000)\noutput = model(input_tensor)\nprint(f\"Output shape: {output.shape}\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:06:20.163963Z","iopub.execute_input":"2025-01-31T17:06:20.164224Z","iopub.status.idle":"2025-01-31T17:06:20.641072Z","shell.execute_reply.started":"2025-01-31T17:06:20.164203Z","shell.execute_reply":"2025-01-31T17:06:20.640058Z"}},"outputs":[{"name":"stdout","text":"Output shape: torch.Size([2, 8, 10000])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}