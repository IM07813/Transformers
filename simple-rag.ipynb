{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "!pip install PyPDF2 sentence-transformers faiss-cpu transformers torch\n",
    "\n",
    "import os\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Preprocess the PDF File\n",
    "def preprocess_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file and split it into chunks.\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing PDF...\")\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    # Split the text into smaller chunks\n",
    "    chunk_size = 200  # Define a reasonable chunk size\n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    print(f\"Extracted {len(chunks)} chunks from the PDF.\")\n",
    "    return chunks\n",
    "\n",
    "# Step 2: Generate Embeddings for Chunks\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for each chunk using a pre-trained Sentence Transformer model.\n",
    "    \"\"\"\n",
    "    print(\"Generating embeddings...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Small, efficient embedding model\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Step 3: Index Embeddings with FAISS\n",
    "def index_embeddings(embeddings):\n",
    "    \"\"\"\n",
    "    Create a FAISS index for efficient similarity search.\n",
    "    \"\"\"\n",
    "    print(\"Indexing embeddings...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)  # Use L2 distance for similarity\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Step 4: Load Pretrained Language Model\n",
    "def load_language_model():\n",
    "    \n",
    "    print(\"Loading language model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Step 5: Answer Questions Using RAG Pipeline\n",
    "def answer_question(question, chunks, embeddings_index, embeddings_model, tokenizer, llm_model):\n",
    "    \n",
    "    print(f\"Answering question: {question}\")\n",
    "\n",
    "    # Step 5.1: Encode the question\n",
    "    question_embedding = embeddings_model.encode([question])\n",
    "\n",
    "    # Step 5.2: Retrieve top-k relevant chunks\n",
    "    k = 3  # Number of relevant chunks to retrieve\n",
    "    distances, indices = embeddings_index.search(question_embedding, k)\n",
    "    retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "\n",
    "    # Step 5.3: Generate an answer using the language model\n",
    "    input_text = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = llm_model.generate(**inputs, max_length=150)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Main Function\n",
    "def main(pdf_file_path):\n",
    "    \"\"\"\n",
    "    Main function to run the RAG pipeline.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the PDF\n",
    "    chunks = preprocess_pdf(pdf_file_path)\n",
    "\n",
    "    # Step 2: Generate embeddings\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "\n",
    "    # Step 3: Index embeddings\n",
    "    embeddings_index = index_embeddings(embeddings)\n",
    "\n",
    "    # Step 4: Load the language model\n",
    "    tokenizer, llm_model = load_language_model()\n",
    "\n",
    "    # Step 5: Answer questions\n",
    "    while True:\n",
    "        question = input(\"Enter your question (type 'exit' to quit): \")\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        answer = answer_question(\n",
    "            question=question,\n",
    "            chunks=chunks,\n",
    "            embeddings_index=embeddings_index,\n",
    "            embeddings_model=SentenceTransformer('all-MiniLM-L6-v2'),\n",
    "            tokenizer=tokenizer,\n",
    "            llm_model=llm_model\n",
    "        )\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "\n",
    "# Run the Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Upload a PDF file in Google Colab\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    pdf_file_path = list(uploaded.keys())[0]\n",
    "\n",
    "    # Run the main function\n",
    "    main(pdf_file_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
